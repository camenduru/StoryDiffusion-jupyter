{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/StoryDiffusion-jupyter/blob/main/test.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/StoryDiffusion-hf\n",
        "%cd /content/StoryDiffusion-hf\n",
        "!pip install -q xformers==0.0.25 diffusers==0.25.0 accelerate omegaconf peft gradio\n",
        "\n",
        "from email.policy import default\n",
        "from json import encoder\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from utils.gradio_utils import is_torch2_available\n",
        "if is_torch2_available():\n",
        "    from utils.gradio_utils import AttnProcessor2_0 as AttnProcessor\n",
        "else:\n",
        "    from utils.gradio_utils import AttnProcessor\n",
        "import diffusers\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from utils import PhotoMakerStableDiffusionXLPipeline\n",
        "from diffusers import DDIMScheduler\n",
        "import torch.nn.functional as F\n",
        "from utils.gradio_utils import cal_attn_mask_xl\n",
        "import copy\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from diffusers.utils import load_image\n",
        "from utils.utils import get_comic\n",
        "from utils.style_template import styles\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def get_image_path_list(folder_name):\n",
        "    image_basename_list = os.listdir(folder_name)\n",
        "    image_path_list = sorted([os.path.join(folder_name, basename) for basename in image_basename_list])\n",
        "    return image_path_list\n",
        "\n",
        "class SpatialAttnProcessor2_0(torch.nn.Module):\n",
        "    r\"\"\"\n",
        "    Attention processor for IP-Adapater for PyTorch 2.0.\n",
        "    Args:\n",
        "        hidden_size (`int`):\n",
        "            The hidden size of the attention layer.\n",
        "        cross_attention_dim (`int`):\n",
        "            The number of channels in the `encoder_hidden_states`.\n",
        "        text_context_len (`int`, defaults to 77):\n",
        "            The context length of the text features.\n",
        "        scale (`float`, defaults to 1.0):\n",
        "            the weight scale of image prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size = None, cross_attention_dim=None,id_length = 4,device = \"cuda\",dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
        "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cross_attention_dim = cross_attention_dim\n",
        "        self.total_length = id_length + 1\n",
        "        self.id_length = id_length\n",
        "        self.id_bank = {}\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        attn,\n",
        "        hidden_states,\n",
        "        encoder_hidden_states=None,\n",
        "        attention_mask=None,\n",
        "        temb=None):\n",
        "        global total_count,attn_count,cur_step,mask1024,mask4096\n",
        "        global sa32, sa64\n",
        "        global write\n",
        "        global height,width\n",
        "        global num_steps\n",
        "        if write:\n",
        "            self.id_bank[cur_step] = [hidden_states[:self.id_length], hidden_states[self.id_length:]]\n",
        "        else:\n",
        "            encoder_hidden_states = torch.cat((self.id_bank[cur_step][0].to(self.device),hidden_states[:1],self.id_bank[cur_step][1].to(self.device),hidden_states[1:]))\n",
        "        if cur_step <=1:\n",
        "            hidden_states = self.__call2__(attn, hidden_states,None,attention_mask,temb)\n",
        "        else:\n",
        "            random_number = random.random()\n",
        "            if cur_step <0.4 * num_steps:\n",
        "                rand_num = 0.3\n",
        "            else:\n",
        "                rand_num = 0.1\n",
        "            if random_number > rand_num:\n",
        "                if not write:\n",
        "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
        "                        attention_mask = mask1024[mask1024.shape[0] // self.total_length * self.id_length:]\n",
        "                    else:\n",
        "                        attention_mask = mask4096[mask4096.shape[0] // self.total_length * self.id_length:]\n",
        "                else:\n",
        "                    # print(self.total_length,self.id_length,hidden_states.shape,(height//32) * (width//32))\n",
        "                    if hidden_states.shape[1] == (height//32) * (width//32):\n",
        "                        attention_mask = mask1024[:mask1024.shape[0] // self.total_length * self.id_length,:mask1024.shape[0] // self.total_length * self.id_length]\n",
        "                    else:\n",
        "                        attention_mask = mask4096[:mask4096.shape[0] // self.total_length * self.id_length,:mask4096.shape[0] // self.total_length * self.id_length]\n",
        "                hidden_states = self.__call1__(attn, hidden_states,encoder_hidden_states,attention_mask,temb)\n",
        "            else:\n",
        "                hidden_states = self.__call2__(attn, hidden_states,None,attention_mask,temb)\n",
        "        attn_count +=1\n",
        "        if attn_count == total_count:\n",
        "            attn_count = 0\n",
        "            cur_step += 1\n",
        "            mask1024,mask4096 = cal_attn_mask_xl(self.total_length,self.id_length,sa32,sa64,height,width, device=self.device, dtype= self.dtype)\n",
        "\n",
        "        return hidden_states\n",
        "    def __call1__(\n",
        "        self,\n",
        "        attn,\n",
        "        hidden_states,\n",
        "        encoder_hidden_states=None,\n",
        "        attention_mask=None,\n",
        "        temb=None,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        if attn.spatial_norm is not None:\n",
        "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
        "        input_ndim = hidden_states.ndim\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            total_batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(total_batch_size, channel, height * width).transpose(1, 2)\n",
        "        total_batch_size,nums_token,channel = hidden_states.shape\n",
        "        img_nums = total_batch_size//2\n",
        "        hidden_states = hidden_states.view(-1,img_nums,nums_token,channel).reshape(-1,img_nums * nums_token,channel)\n",
        "\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is None:\n",
        "            encoder_hidden_states = hidden_states  # B, N, C\n",
        "        else:\n",
        "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,nums_token,channel).reshape(-1,(self.id_length+1) * nums_token,channel)\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "\n",
        "\n",
        "        inner_dim = key.shape[-1]\n",
        "        head_dim = inner_dim // attn.heads\n",
        "\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "        hidden_states = F.scaled_dot_product_attention(\n",
        "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "        )\n",
        "\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(total_batch_size, -1, attn.heads * head_dim)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "        hidden_states = attn.to_out[0](hidden_states)\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(total_batch_size, channel, height, width)\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "\n",
        "        return hidden_states\n",
        "    def __call2__(\n",
        "        self,\n",
        "        attn,\n",
        "        hidden_states,\n",
        "        encoder_hidden_states=None,\n",
        "        attention_mask=None,\n",
        "        temb=None):\n",
        "        residual = hidden_states\n",
        "\n",
        "        if attn.spatial_norm is not None:\n",
        "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
        "\n",
        "        input_ndim = hidden_states.ndim\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        batch_size, sequence_length, channel = (\n",
        "            hidden_states.shape\n",
        "        )\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is None:\n",
        "            encoder_hidden_states = hidden_states  # B, N, C\n",
        "        else:\n",
        "            encoder_hidden_states = encoder_hidden_states.view(-1,self.id_length+1,sequence_length,channel).reshape(-1,(self.id_length+1) * sequence_length,channel)\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "\n",
        "        inner_dim = key.shape[-1]\n",
        "        head_dim = inner_dim // attn.heads\n",
        "\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        hidden_states = F.scaled_dot_product_attention(\n",
        "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "        )\n",
        "\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "\n",
        "        hidden_states = attn.to_out[0](hidden_states)\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
        "\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual\n",
        "\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "def set_attention_processor(unet,id_length,is_ipadapter = False):\n",
        "    global total_count\n",
        "    total_count = 0\n",
        "    attn_procs = {}\n",
        "    for name in unet.attn_processors.keys():\n",
        "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "        if name.startswith(\"mid_block\"):\n",
        "            hidden_size = unet.config.block_out_channels[-1]\n",
        "        elif name.startswith(\"up_blocks\"):\n",
        "            block_id = int(name[len(\"up_blocks.\")])\n",
        "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
        "        elif name.startswith(\"down_blocks\"):\n",
        "            block_id = int(name[len(\"down_blocks.\")])\n",
        "            hidden_size = unet.config.block_out_channels[block_id]\n",
        "        if cross_attention_dim is None:\n",
        "            if name.startswith(\"up_blocks\") :\n",
        "                attn_procs[name] = SpatialAttnProcessor2_0(id_length = id_length)\n",
        "                total_count +=1\n",
        "            else:    \n",
        "                attn_procs[name] = AttnProcessor()\n",
        "        else:\n",
        "            if is_ipadapter:\n",
        "                attn_procs[name] = IPAttnProcessor2_0(\n",
        "                    hidden_size=hidden_size,\n",
        "                    cross_attention_dim=cross_attention_dim,\n",
        "                    scale=1,\n",
        "                    num_tokens=4,\n",
        "                ).to(unet.device, dtype=torch.float16)\n",
        "            else:\n",
        "                attn_procs[name] = AttnProcessor()\n",
        "\n",
        "    unet.set_attn_processor(copy.deepcopy(attn_procs))\n",
        "    print(\"successsfully load paired self-attention\")\n",
        "    print(f\"number of the processor : {total_count}\")\n",
        "\n",
        "def apply_style_positive(style_name: str, positive: str):\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return p.replace(\"{prompt}\", positive) \n",
        "\n",
        "def apply_style(style_name: str, positives: list, negative: str = \"\"):\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return [p.replace(\"{prompt}\", positive) for positive in positives], n + ' ' + negative\n",
        "\n",
        "def process_generation(_sd_type,_model_type,_upload_images, _num_steps,style_name, _Ip_Adapter_Strength ,_style_strength_ratio, guidance_scale, seed_,  sa32_, sa64_, id_length_,  general_prompt, negative_prompt,prompt_array,G_height,G_width,_comic_type):\n",
        "    _model_type = \"Photomaker\" if _model_type == \"Using Ref Images\" else \"original\"\n",
        "    if _model_type == \"Photomaker\" and \"img\" not in general_prompt:\n",
        "        print(f\"Please add the triger word \\\" img \\\"  behind the class word you want to customize, such as: man img or woman img\")\n",
        "    if _upload_images is None and _model_type != \"original\":\n",
        "        print(f\"Cannot find any input face image!\")\n",
        "    if len(prompt_array.splitlines()) > 10:\n",
        "        print(f\"No more than 10 prompts in huggface demo for Speed! But found {len(prompt_array.splitlines())} prompts!\")\n",
        "    global sa32, sa64,id_length,total_length,attn_procs,unet,cur_model_type,device\n",
        "    global num_steps\n",
        "    global write\n",
        "    global cur_step,attn_count\n",
        "    global height,width\n",
        "    height = G_height\n",
        "    width = G_width\n",
        "    global pipe2,pipe4\n",
        "    global sd_model_path,models_dict\n",
        "    sd_model_path = models_dict[_sd_type]\n",
        "    num_steps =_num_steps\n",
        "    use_safe_tensor = True\n",
        "    if  style_name == \"(No style)\":\n",
        "        sd_model_path = models_dict[\"RealVision\"]\n",
        "    if _model_type == \"original\":\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(sd_model_path, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(device)\n",
        "        pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
        "        set_attention_processor(pipe.unet,id_length_,is_ipadapter = False)\n",
        "    elif _model_type == \"Photomaker\":\n",
        "        if _sd_type != \"RealVision\" and style_name != \"(No style)\":\n",
        "            pipe = pipe2.to(device)\n",
        "            pipe.id_encoder.to(device)\n",
        "            set_attention_processor(pipe.unet,id_length_,is_ipadapter = False)\n",
        "        else:\n",
        "            pipe = pipe4.to(device)\n",
        "            pipe.id_encoder.to(device)\n",
        "            set_attention_processor(pipe.unet,id_length_,is_ipadapter = False)\n",
        "    else:\n",
        "        raise NotImplementedError(\"You should choice between original and Photomaker!\",f\"But you choice {_model_type}\")\n",
        "\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
        "    cur_model_type = _sd_type+\"-\"+_model_type+\"\"+str(id_length_)\n",
        "    if _model_type != \"original\":\n",
        "        input_id_images = []\n",
        "        for img in _upload_images:\n",
        "            print(img)\n",
        "            input_id_images.append(load_image(img))\n",
        "    prompts = prompt_array.splitlines()\n",
        "    start_merge_step = int(float(_style_strength_ratio) / 100 * _num_steps)\n",
        "    if start_merge_step > 30:\n",
        "        start_merge_step = 30\n",
        "    print(f\"start_merge_step:{start_merge_step}\")\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(seed_)\n",
        "    sa32, sa64 =  sa32_, sa64_\n",
        "    id_length = id_length_\n",
        "    clipped_prompts = prompts[:]\n",
        "    prompts = [general_prompt + \",\" + prompt if \"[NC]\" not in prompt else prompt.replace(\"[NC]\",\"\")  for prompt in clipped_prompts]\n",
        "    prompts = [prompt.rpartition('#')[0] if \"#\" in prompt else prompt for prompt in prompts]\n",
        "    print(prompts)\n",
        "    id_prompts = prompts[:id_length]\n",
        "    real_prompts = prompts[id_length:]\n",
        "    torch.cuda.empty_cache()\n",
        "    write = True\n",
        "    cur_step = 0\n",
        "\n",
        "    attn_count = 0\n",
        "    id_prompts, negative_prompt = apply_style(style_name, id_prompts, negative_prompt)\n",
        "    setup_seed(seed_)\n",
        "    total_results = []\n",
        "    if _model_type == \"original\":\n",
        "        id_images = pipe(id_prompts, num_inference_steps=_num_steps, guidance_scale=guidance_scale,  height = height, width = width,negative_prompt = negative_prompt,generator = generator).images\n",
        "    elif _model_type == \"Photomaker\":\n",
        "        id_images = pipe(id_prompts,input_id_images=input_id_images, num_inference_steps=_num_steps, guidance_scale=guidance_scale, start_merge_step = start_merge_step, height = height, width = width,negative_prompt = negative_prompt,generator = generator).images\n",
        "    else: \n",
        "        raise NotImplementedError(\"You should choice between original and Photomaker!\",f\"But you choice {_model_type}\")\n",
        "    total_results = id_images + total_results\n",
        "    yield total_results\n",
        "    real_images = []\n",
        "    write = False\n",
        "    for real_prompt in real_prompts:\n",
        "        setup_seed(seed_)\n",
        "        cur_step = 0\n",
        "        real_prompt = apply_style_positive(style_name, real_prompt)\n",
        "        if _model_type == \"original\":   \n",
        "            real_images.append(pipe(real_prompt,  num_inference_steps=_num_steps, guidance_scale=guidance_scale,  height = height, width = width,negative_prompt = negative_prompt,generator = generator).images[0])\n",
        "        elif _model_type == \"Photomaker\":      \n",
        "            real_images.append(pipe(real_prompt, input_id_images=input_id_images, num_inference_steps=_num_steps, guidance_scale=guidance_scale,  start_merge_step = start_merge_step, height = height, width = width,negative_prompt = negative_prompt,generator = generator).images[0])\n",
        "        else:\n",
        "            raise NotImplementedError(\"You should choice between original and Photomaker!\",f\"But you choice {_model_type}\")\n",
        "        total_results = [real_images[-1]] + total_results\n",
        "        yield total_results\n",
        "    if _comic_type != \"No typesetting (default)\":\n",
        "        captions= prompt_array.splitlines()\n",
        "        captions = [caption.replace(\"[NC]\",\"\") for caption in captions]\n",
        "        captions = [caption.split('#')[-1] if \"#\" in caption else caption for caption in captions]\n",
        "        from PIL import ImageFont\n",
        "        total_results = get_comic(id_images + real_images, _comic_type,captions= captions,font=ImageFont.truetype(\"./fonts/Inkfree.ttf\", int(45))) + total_results\n",
        "    if _model_type == \"Photomaker\":\n",
        "        pipe = pipe2.to(\"cpu\")\n",
        "        pipe.id_encoder.to(\"cpu\")\n",
        "        set_attention_processor(pipe.unet,id_length_,is_ipadapter = False)\n",
        "    yield total_results\n",
        "\n",
        "def array2string(arr):\n",
        "    stringtmp = \"\"\n",
        "    for i,part in enumerate(arr):\n",
        "        if i != len(arr)-1:\n",
        "            stringtmp += part +\"\\n\"\n",
        "        else:\n",
        "            stringtmp += part\n",
        "\n",
        "    return stringtmp\n",
        "\n",
        "DEFAULT_STYLE_NAME = \"Japanese Anime\"\n",
        "models_dict = {\"RealVision\": \"SG161222/RealVisXL_V4.0\" , \"Unstable\": \"stablediffusionapi/sdxl-unstable-diffusers-y\"}\n",
        "photomaker_path =  hf_hub_download(repo_id=\"TencentARC/PhotoMaker\", filename=\"photomaker-v1.bin\", repo_type=\"model\")\n",
        "\n",
        "global models_dict\n",
        "global attn_count, total_count, id_length, total_length, cur_step, cur_model_type\n",
        "global write\n",
        "global sa32, sa64\n",
        "global height, width\n",
        "global attn_procs, unet\n",
        "global sd_model_path\n",
        "\n",
        "attn_count = 0\n",
        "total_count = 0\n",
        "cur_step = 0\n",
        "id_length = 4\n",
        "total_length = 5\n",
        "cur_model_type = \"\"\n",
        "device=\"cuda\"\n",
        "attn_procs = {}\n",
        "write = False\n",
        "sa32 = 0.5\n",
        "sa64 = 0.5\n",
        "height = 768\n",
        "width = 768\n",
        "sd_model_path = models_dict[\"Unstable\"]\n",
        "use_safetensors= False\n",
        "\n",
        "pipe2 = PhotoMakerStableDiffusionXLPipeline.from_pretrained(models_dict[\"Unstable\"], torch_dtype=torch.float16, use_safetensors=use_safetensors)\n",
        "pipe2 = pipe2.to(\"cpu\")\n",
        "pipe2.load_photomaker_adapter(os.path.dirname(photomaker_path), subfolder=\"\", weight_name=os.path.basename(photomaker_path), trigger_word=\"img\")\n",
        "pipe2 = pipe2.to(\"cpu\")\n",
        "pipe2.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
        "pipe2.fuse_lora()\n",
        "pipe4 = PhotoMakerStableDiffusionXLPipeline.from_pretrained(models_dict[\"RealVision\"], torch_dtype=torch.float16, use_safetensors=True)\n",
        "pipe4 = pipe4.to(\"cpu\")\n",
        "pipe4.load_photomaker_adapter(os.path.dirname(photomaker_path), subfolder=\"\", weight_name=os.path.basename(photomaker_path), trigger_word=\"img\")\n",
        "pipe4 = pipe4.to(\"cpu\")\n",
        "pipe4.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
        "pipe4.fuse_lora()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sd_type=\"Unstable\"\n",
        "num_steps = 50\n",
        "Ip_Adapter_Strength=0.5\n",
        "style_strength_ratio=20\n",
        "guidance_scale=5\n",
        "comic_type=\"Classic Comic Style\"\n",
        "seed_=1\n",
        "sa32_=0.5\n",
        "sa64_=0.5\n",
        "id_length_=3\n",
        "general_prompt=\"a woman img, wearing a white T-shirt, blue loose hair\"\n",
        "negative_prompt=\"bad anatomy, bad hands, missing fingers, extra fingers, three hands, three legs, bad arms, missing legs, missing arms, poorly drawn face, bad face, fused face, cloned face, three crus, fused feet, fused thigh, extra crus, ugly fingers, horn, cartoon, cg, 3d, unreal, animate, amputation, disconnected limbs\"\n",
        "prompt_array=array2string([\"wake up in the bed\",\n",
        "                                \"have breakfast\",\n",
        "                                \"is on the road, go to company\",\n",
        "                                \"work in the company\",\n",
        "                                \"Take a walk next to the company at noon\",\n",
        "                                \"lying in bed at night\"])\n",
        "style=\"Japanese Anime\"\n",
        "model_type=\"Using Ref Images\"\n",
        "files=get_image_path_list('./examples/taylor')\n",
        "G_height=768\n",
        "G_width=768\n",
        "images = process_generation(sd_type, model_type, files, num_steps,style, Ip_Adapter_Strength,style_strength_ratio, guidance_scale, seed_, sa32_, sa64_, id_length_, general_prompt, negative_prompt, prompt_array,G_height,G_width,comic_type)\n",
        "\n",
        "def save_images_and_collect_paths(images):\n",
        "    image_objects = []\n",
        "    for index, image in enumerate(images):\n",
        "        image_objects.append(image)\n",
        "    return image_objects\n",
        "\n",
        "image_objects = save_images_and_collect_paths(images)\n",
        "\n",
        "def remove_duplicates_and_save(images, save_path):\n",
        "    unique_images_paths = []\n",
        "    unique_images = set()\n",
        "    index = 0\n",
        "    for row in images:\n",
        "        for img in row:\n",
        "            img_data = img.tobytes()\n",
        "            if img_data not in unique_images:\n",
        "                unique_images.add(img_data)\n",
        "                img_path = os.path.join(save_path, f\"unique_{index}.png\")\n",
        "                img.save(img_path)\n",
        "                unique_images_paths.append(img_path)\n",
        "                index += 1\n",
        "    return unique_images_paths\n",
        "\n",
        "save_path = \"/content\"\n",
        "unique_images_paths = remove_duplicates_and_save(image_objects, save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
